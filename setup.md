---
layout: page
title: Setup
---

~~This workshop is designed to be run on pre-imaged Amazon Web Services 
(AWS) instances. For information about how to
use the workshop materials, see the 
[setup instructions](http://www.datacarpentry.org/genomics-workshop/setup.html) on the main workshop page.~~

This workshop is adapted to work on [CURC Alpine](https://curc.readthedocs.io/en/latest/clusters/alpine/index.html) for the graduate class CM580A3 at Colorado State University. All instructions that follow apply specifically to CSU users, and were determined in April of 2023. Some of the details may change with system administration changes.

## Getting an account for CSU users


### Step 1. You must have a valid CSU NetID and Duo two-factor authentication enabled

You must have a valid CSU NetID and Duo two-factor authentication enabled to apply for an account on Alpine through CSU.

### Step 2. Using DUO and NetID to request an account

**To get started, request a CU-Boulder account.** Note that you must use DUO to authenticate through this form. In the password field, type your CSU NetID password, then a comma, then the word “push” or the DUO key generated by your device.

For example, if your password was ilovealpine and your DUO key is 123456, you would type:

`ilovealpine,push`

OR

`ilovealpine,123456`

### Step 3. Request RMACC account.

[Request form for an account on the system](https://it.colostate.edu/research-computing-and-cyberinfrastructure/compute/get-started-with-alpine/)

Follow the instructions starting at **You will then be asked to provide information about your account.** (right above the screenshot).

### Step 4. Email Verification.

You should receive an immediate, open ticket confirmation from rc-help@colostate.edu.  The account will created by the system administrator and **you'll be notified by a second email.**

Once receiving that notification, you can log in to the system

## Logging on once your account is created

There are two ways to connect and use the Alpine system. One is through OnDemand (described below) and the other is a direct ssh from your computer.

 1. https://ondemand-rmacc.rc.colorado.edu/
    - operates through tabs in web browser
    - can remember password, duo authentication 
    - graphical file browser, with upload/download options
    - file editor and terminal windows
 2. ssh from your terminal
    - `ssh -l eid@colostate.edu rc.colorado.edu`
    - sometimes it works better
    - sometimes it's the only way to get on the system
    - command line only
    - need password,push DUO every time


### OnDemand Walkthrough

## Setting up your account 

### Setup script 

Open an Alpine_shell and do the following 
**unless you're already set up with conda
 on Alpine**

For users who are already set up with 
conda, you can look at the script to see
 what it does. 

```
cd /projects/$USER
git clone https://github.com/Colorado-State-University-CMB/CM580A3-Intro-to-qCMB-2023 
cd CM580A3-Intro-to-qCMB-2023/10_alpine
bash setup.sh
```

This script configures conda for you and
 adds slurm aliases that make things 
easier. It also adds the following line 
to your `.bashrc` in order to activate conda 
when you open a shell. 

`source /curc/sw/anaconda3/latest`

This line makes conda available, and 
activates a default environment called "base". 
The word `(base)` is prepended to your prompt
to indicate which environment is active. 

## Super computing concepts 

### Submitting jobs

A cluster computer is more complicated than just logging on to a super machine. 
It allows us to distribute our computational workload across nodes and CPUs. 

That means that all the computationally heavy work must be done in a script that is passed to a job scheduler called SLURM-the system will then distribute the work to its 10s of thousands of resources using configuration variables added to the script. 

Generally, the scripts are submitted using a command called "sbatch", like so:

```
sbatch scriptname.sh
# as opposed to 
bash scriptname.sh
```

... where scriptname.sh contains
 special commented lines that configure 
the job resources. 
They look like

```
#SBATCH --ntasks=4
#SBATCH --nodes=1
#SBATCH --time=1:00:00
```
These "SBATCH" headers request
 4 CPUs on one node, for 1 hour.

All "SBATCH" headers go beneath a line
 called shebang, and must come before 
the commands of the script. 

```
#!/usr/bin/env bash
#SBATCH --ntasks=4
#SBATCH --nodes=1
#SBATCH --time=1:00:00

Command1

Command2
```

### Installing software

Installing software can 
require a lot of time in order
to download and compile source code.

Therefore, a special job is started in
order to isolate resources.

Initiate the compile session:

```
acompile --ntasks=4
```

You may have to wait for a few seconds.
 When it is ready, your prompt will
 return to you, but the host name will
be different.

### Building environments for the variant calling workflow

We will install programs into a few 
different environments in order to
better use class time. Normally, all programs
are installed into a single environment.

```
conda create env -n qc-trim fastq trimmomatic
```

This message is OK: **frozen solve failed**

The installation tries different "solves",
 but there is only an issue if the command quits.

After this step, conda will have figured
out what the requested programs need, and
will ask if you want to proceed.

Hit **enter** to proceed with downloading
 and installation.

